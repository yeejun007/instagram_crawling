{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  크롤링 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting insta_crawling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile insta_crawling.py\n",
    "import requests\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from scrapy.http import TextResponse\n",
    "import urllib.request\n",
    "import os \n",
    "import getpass\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "class instagram_crawling():\n",
    "    \n",
    "    def __init__(self, headless=False):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--window-size=1600,1080\")\n",
    "        if headless == True:\n",
    "            chrome_options.add_argument('headless')\n",
    "        \n",
    "        login_url = \"https://www.instagram.com/accounts/login/?source=auth_switcher\"\n",
    "        \n",
    "        self.driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        self.driver.get(login_url)\n",
    "        \n",
    "        \n",
    "    # element의 렌더링을 기다려야 하는 경우, 렌더링 될때까지 재귀적으로 함수 실행  \n",
    "    def check_response(self, webdriver, selector, func , start_time):\n",
    "        try:\n",
    "            if func == 'login':\n",
    "                result = webdriver.find_element_by_css_selector(selector)\n",
    "                \n",
    "            elif func == 'input_keyword':\n",
    "                if time.time() - start_time > 10:\n",
    "                    print(\"=========== no response in function : {} ===========\".format(func))\n",
    "                    return False\n",
    "            \n",
    "                result = webdriver.find_elements_by_css_selector(selector)\n",
    "                while len(result) == 0:\n",
    "                    return self.check_response(webdriver, selector, func, start_time)\n",
    "                \n",
    "            elif func == 'initial_crawling':\n",
    "                if time.time() - start_time > 10:\n",
    "                    print(\"=========== no response in function : {} ===========\".format(func))\n",
    "                    return False\n",
    "            \n",
    "                result = webdriver.find_elements_by_css_selector(selector)\n",
    "                while len(result) != 8:\n",
    "                    return self.check_response(webdriver, selector, func, start_time)\n",
    "                \n",
    "            return result\n",
    "        \n",
    "        except Exception:\n",
    "            # func = 'login'인 경우 element를 찾지 못하면 에러처리로 들어온다\n",
    "            \n",
    "            if time.time() - start_time > 5:     \n",
    "                print(\"=========== no response in function : {} ===========\".format(func))\n",
    "                return False\n",
    "            \n",
    "            else:\n",
    "                return self.check_response(webdriver, selector, func, start_time)\n",
    "        \n",
    "            \n",
    "    \n",
    "    # 로그인 함수 (seleniun webdriver)\n",
    "    def login(self):\n",
    "        my_id = input('id를 입력하세요: ')\n",
    "        my_password = getpass.getpass('password를 입력하세요: ')\n",
    "\n",
    "        self.driver.find_element_by_css_selector('._2hvTZ.pexuQ.zyHYP[type=text]').send_keys(my_id)\n",
    "        self.driver.find_element_by_css_selector('._2hvTZ.pexuQ.zyHYP[type=password]').send_keys(my_password)\n",
    "        self.driver.find_element_by_css_selector('.sqdOP.L3NKy.y3zKF[type=submit]').click()\n",
    "\n",
    "\n",
    "        # 알림설정 하라는 modal창이 뜨는경우 '나중에 하기'를 클릭하는 코드\n",
    "        start_time = time.time()\n",
    "        alert_modal = self.check_response(self.driver, 'body > div.RnEpo.Yx5HN > div > div', 'login', start_time)\n",
    "        if alert_modal == False:\n",
    "            try:\n",
    "                wrong_access = webdriver.find_element_by_css_selectortor('#slfErrorAlert')\n",
    "                \n",
    "            except Exception:\n",
    "                print('알림설정창 안뜸')\n",
    "                return\n",
    "        \n",
    "        self.driver.find_element_by_css_selector('body > div.RnEpo.Yx5HN > div > div > div.mt3GC > button.aOOlW.HoLwm').click()\n",
    "\n",
    "\n",
    "\n",
    "    # 검색어를 입력하고 게시물이 가장 많은 검색결과를 클릭 (seleniun webdriver)\n",
    "    def input_keyword(self, word):\n",
    "        keyword = word\n",
    "\n",
    "        self.driver.find_element_by_css_selector('#react-root > section > nav > div._8MQSO.Cx7Bp > div > div > div.LWmhU._0aCwM > input').send_keys(keyword)\n",
    "\n",
    "        # 키워드 입력하고, 키워드에 대한 검색 리스트가 뜨는데 걸리는 시간만큼 기다려준다\n",
    "        start_time = time.time()\n",
    "        search_list = self.check_response(self.driver, '#react-root > section > nav > div._8MQSO.Cx7Bp > div > div > div.LWmhU._0aCwM > div:nth-child(4) > div.drKGC > div > a', 'input_keyword', start_time)\n",
    "        if search_list == False:\n",
    "            print('================ input_keyword rendering error ================')\n",
    "            self.driver.quit()\n",
    "        \n",
    "        # 서칭된 a 엘리먼트들 중에 게시물 갯수가 가장 많은 a 엘리먼트를 뽑는다\n",
    "        number_list = []\n",
    "\n",
    "        for element in search_list:\n",
    "            splited_element = element.text.split('게시물')\n",
    "            if len(splited_element) == 2:\n",
    "                number_list.append(int(re.sub(\",\", \"\", splited_element[1])))\n",
    "            else:\n",
    "                number_list.append(0)\n",
    "\n",
    "        max_number = max(number_list)\n",
    "        click_index = number_list.index(max_number)\n",
    "\n",
    "        search_list[click_index].click()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 처음 렌더링된 화면의 24개의 게시물을 크롤링하는 함수\n",
    "    def initial_crawling(self):\n",
    "        \n",
    "        # 검색 키워드를 클릭 한 이후 게시물 렌더링이 완료되었는지 확인\n",
    "        start_time = time.time()\n",
    "        result = self.check_response(self.driver, '#react-root > section > main > article > div:nth-child(3) > div > div', 'initial_crawling', start_time)\n",
    "        if result == False:\n",
    "            print('================ initial_crawling rendering error ================')\n",
    "            self.driver.quit()\n",
    "        \n",
    "        urls = self.driver.find_elements_by_xpath('//*[@id=\"react-root\"]/section/main/article/div[2]/div/div/div/a')\n",
    "        urls = [url.get_attribute(\"href\") for url in urls]\n",
    "        \n",
    "        image_urls = self.driver.find_elements_by_xpath(\n",
    "        '//*[@id=\"react-root\"]/section/main/article/div/div/div/div/a/div/div/img')\n",
    "        image_urls = [url.get_attribute(\"src\") for url in image_urls]\n",
    "\n",
    "        hash_tag_df = pd.DataFrame(columns=['time', 'hash_tag'])\n",
    "\n",
    "        for i in range(len(urls)):\n",
    "            req = requests.get(urls[i])\n",
    "            response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")\n",
    "            hash_tag_list = response.css(\"meta[property='instapp:hashtags']\")\n",
    "            hash_tag_list = ['#' + hash_tag.attrib['content'] for hash_tag in hash_tag_list]\n",
    "            hash_tags = ', '.join(hash_tag_list)\n",
    "\n",
    "            data = [\n",
    "                {'time': 0, 'hash_tag': hash_tags, 'image_url': image_urls[i]}\n",
    "            ]\n",
    "            df = pd.DataFrame(data)\n",
    "            hash_tag_df = hash_tag_df.append(df)\n",
    "\n",
    "        return hash_tag_df.reset_index().drop(columns=['index'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 스크롤내리면서 12개씩 추가되는 게시물을 크롤링하는 함수 (두번째만 실행) - chrome 전체화면 기준\n",
    "    # 현재 브라우저 화면 크기에 따라 추가적으로 불러오는 div 갯수가 달라짐\n",
    "    # 전체화면 기준 div의 urls 총 갯수 = 24 -> 36 -> 45 -> 45 -> ...\n",
    "    def second_crawling(self):\n",
    "        \n",
    "        hash_tag_df = pd.DataFrame(columns=['time', 'hash_tag'])\n",
    "        \n",
    "        for i in range(0, 1):\n",
    "            self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "            time.sleep(1.5)\n",
    "\n",
    "            urls = self.driver.find_elements_by_xpath('//*[@id=\"react-root\"]/section/main/article/div[2]/div/div/div/a')\n",
    "            urls = [url.get_attribute(\"href\") for url in urls]\n",
    "            \n",
    "            image_urls = self.driver.find_elements_by_xpath(\n",
    "                '//*[@id=\"react-root\"]/section/main/article/div/div/div/div/a/div/div/img')\n",
    "            image_urls = [url.get_attribute(\"src\") for url in image_urls]\n",
    "\n",
    "            for i in range(len(urls) - 12, len(urls)):\n",
    "                req = requests.get(urls[i])\n",
    "                response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")\n",
    "                hash_tag_list = response.css(\"meta[property='instapp:hashtags']\")\n",
    "                hash_tag_list = ['#' + hash_tag.attrib['content'] for hash_tag in hash_tag_list]\n",
    "                hash_tags = ', '.join(hash_tag_list)\n",
    "\n",
    "                data = [\n",
    "                    {'time': 0, 'hash_tag': hash_tags, 'image_url': image_urls[i]}\n",
    "                ]\n",
    "                df = pd.DataFrame(data)\n",
    "                hash_tag_df = hash_tag_df.append(df)\n",
    "\n",
    "        return hash_tag_df.reset_index().drop(columns=['index'])\n",
    "\n",
    "\n",
    "\n",
    "    # 세번째부터는 이 함수 반복하기 - 게시물 9개씩 크롤링\n",
    "    def repeat_crawling(self):\n",
    "        \n",
    "        hash_tag_df = pd.DataFrame(columns=['time', 'hash_tag'])\n",
    "        \n",
    "        for i in range(0, 1):\n",
    "            self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "            time.sleep(1.5)\n",
    "\n",
    "            urls = self.driver.find_elements_by_xpath('//*[@id=\"react-root\"]/section/main/article/div[2]/div/div/div/a')\n",
    "            urls = [url.get_attribute(\"href\") for url in urls]\n",
    "            \n",
    "            image_urls = self.driver.find_elements_by_xpath(\n",
    "                '//*[@id=\"react-root\"]/section/main/article/div/div/div/div/a/div/div/img')\n",
    "            image_urls = [url.get_attribute(\"src\") for url in image_urls]\n",
    "\n",
    "            for i in range(len(urls) - 9, len(urls)):\n",
    "                req = requests.get(urls[i])\n",
    "                response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")\n",
    "                hash_tag_list = response.css(\"meta[property='instapp:hashtags']\")\n",
    "                hash_tag_list = ['#' + hash_tag.attrib['content'] for hash_tag in hash_tag_list]\n",
    "                hash_tags = ','.join(hash_tag_list)\n",
    "\n",
    "                data = [\n",
    "                    {'time': 0, 'hash_tag': hash_tags, 'image_url': image_urls[i]}\n",
    "                ]\n",
    "                df = pd.DataFrame(data)\n",
    "                hash_tag_df = hash_tag_df.append(df)\n",
    "\n",
    "        return hash_tag_df.reset_index().drop(columns=['index'])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# instagram_crawling 클래스를 이용해서 해시태그를 크롤링하고 전체 데이터프레임을 리턴한다\n",
    "def crawling_start(keyword, repeat_num, headless=False, mongo_save=0):\n",
    "    \"\"\"\n",
    "    3 input arguments.\n",
    "    keyword: \n",
    "        검색어(주제)\n",
    "    repeat_num: \n",
    "        세번째 크롤링 함수의 반복 횟수\n",
    "        ex) repeat_num = 2 -> 데이터 row 갯수 = 24 + 12 + (9 * 2)\n",
    "            repeat_num = 3 -> 데이터 row 갯수 = 24 + 12 + (9 * 3)\n",
    "            repeat_num = 4 -> 데이터 row 갯수 = 24 + 12 + (9 * 4)\n",
    "    mongo_save:\n",
    "        aws 서버의 mongodb 데이터베이스에 저장 할 건지 결정하는 인자\n",
    "        저장 = 1\n",
    "        저장 안함 = 0\n",
    "    \"\"\"\n",
    "    \n",
    "    keyword = keyword\n",
    "    num = repeat_num\n",
    "    result_df = pd.DataFrame(columns=['time', 'hash_tag'])\n",
    "    \n",
    "    insta = instagram_crawling(headless=headless)\n",
    "    \n",
    "    insta.login()\n",
    "    insta.input_keyword(keyword)\n",
    "    \n",
    "    initial_df = insta.initial_crawling()\n",
    "    result_df = pd.concat([result_df, initial_df])\n",
    "    \n",
    "    second_df = insta.second_crawling()\n",
    "    result_df = pd.concat([result_df, second_df])\n",
    "    \n",
    "    # 마지막 크롤링 함수 반복\n",
    "    for i in range(0, num):\n",
    "        try:\n",
    "            df = insta.repeat_crawling()\n",
    "            result_df = pd.concat([result_df, df])\n",
    "        except Exception as exc:\n",
    "            print('에러가 발생했습니다 : ', exc)\n",
    "    \n",
    "    result_df = result_df.reset_index().drop(columns=['index'])\n",
    "    \n",
    "    \n",
    "    # 세번째 인자를 1로 넘겨주면, aws mongodb에 데이터를 저장한다\n",
    "    if mongo_save == 1:\n",
    "        mongo_df_list = result_df.to_dict(\"records\")\n",
    "        client = pymongo.MongoClient('mongodb://root:dss@13.124.100.70:27017')\n",
    "        db = client.insta_crawling\n",
    "        collection = db.data\n",
    "\n",
    "        for i in range(len(mongo_df_list)):\n",
    "            collection.insert(mongo_df_list[i])\n",
    "    \n",
    "    return result_df.drop(columns=['time'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 처리 모듈\n",
    "- 일반인들이 올린 게시물이 아닌, 이상한 광고 게시물의 해시태그 걸러내기\n",
    "- 이미지 url의 이미지 모두 저장해서 google vision api사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "import urllib\n",
    "import os\n",
    "import io\n",
    "from os import walk\n",
    "\n",
    "class spam_filter():\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df \n",
    "    \n",
    "    def make_img_files(self):\n",
    "        if not os.path.exists('./insta_img'):\n",
    "            os.mkdir('insta_img')\n",
    "\n",
    "        for index, row in self.df.iterrows():\n",
    "            urllib.request.urlretrieve(row['image_url'], './insta_img/insta_img_{:02d}.png'.format(index))\n",
    "    \n",
    "    def spam_check(self):\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/song-yeejun/Desktop/Fastcampus/dss01/google_vision_key/vision_key.json'\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    file_list = []\n",
    "    for (dirpath, dirnames, filenames) in walk('./insta_img'):\n",
    "        file_list.extend(filenames)\n",
    "    file_list.sort()\n",
    "    file_list\n",
    "\n",
    "    img_words = []\n",
    "    for index, each_file in enumerate(file_list):\n",
    "        with io.open('./insta_img/{}'.format(each_file), 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        image = vision.types.Image(content=content)\n",
    "        response = client.text_detection(image=image)\n",
    "        texts = response.text_annotations\n",
    "\n",
    "        if len(texts) > 0:\n",
    "            img_words.append(('{:02}'.format(index), texts[0].description))\n",
    "        else:\n",
    "            img_words.append(('{:02}'.format(index), \"\"))\n",
    "\n",
    "    return img_words\n",
    "    \n",
    "\n",
    "def filtering_start(df):\n",
    "    \"\"\"\n",
    "    1 input argument.\n",
    "    df : The result dataframe of crawling_start function that included insta_crawling package.\n",
    "        ex) df = crawling_start()\n",
    "            filtering_start(df)\n",
    "    \"\"\"\n",
    "    \n",
    "    spam_words = ['성인출장삼', '섹타임 출장잡', '예약카톡', '이쁜이들 항시대기중!!!',\n",
    "                  '성인', '출장의사', '19금 성인', '전국출장', '러브출장샵', '상담카톡', '섹타 임', '출장마사지']\n",
    "    \n",
    "    insta_spam = spam_filter(df)\n",
    "    insta_spam.make_img_files()\n",
    "    img_words_list = insta_spam.spam_check()\n",
    "    \n",
    "    for index, words in img_words_list:\n",
    "        check = [each_word in spam_words for each_word in words.split('\\n')]\n",
    "        if True in check:\n",
    "            print(index)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
