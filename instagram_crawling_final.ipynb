{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from scrapy.http import TextResponse\n",
    "import seaborn as sns\n",
    "import getpass\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그인 함수 (seleniun webdriver)\n",
    "\n",
    "def login():\n",
    "    login_url = \"https://www.instagram.com/accounts/login/?source=auth_switcher\"\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(login_url)\n",
    "    \n",
    "    my_id = input('id를 입력하세요: ')\n",
    "    my_password = getpass.getpass('password를 입력하세요: ')\n",
    "\n",
    "    driver.find_element_by_css_selector('._2hvTZ.pexuQ.zyHYP[type=text]').send_keys(my_id)\n",
    "    driver.find_element_by_css_selector('._2hvTZ.pexuQ.zyHYP[type=password]').send_keys(my_password)\n",
    "    driver.find_element_by_css_selector('.sqdOP.L3NKy.y3zKF[type=submit]').click()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    # 알림설정 하라는 modal창이 뜨는경우 '나중에 하기'를 클릭하는 코드\n",
    "    alert_modal = driver.find_element_by_css_selector('body > div.RnEpo.Yx5HN > div > div')\n",
    "    if alert_modal:\n",
    "        driver.find_element_by_css_selector('body > div.RnEpo.Yx5HN > div > div > div.mt3GC > button.aOOlW.HoLwm').click()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색어를 입력하고 게시물이 가장 많은 검색결과를 클릭 (seleniun webdriver)\n",
    "\n",
    "def crawling_keyword(word):\n",
    "    keyword = word\n",
    "\n",
    "    driver.find_element_by_css_selector('#react-root > section > nav > div._8MQSO.Cx7Bp > div > div > div.LWmhU._0aCwM > input').send_keys(keyword)\n",
    "\n",
    "    # 키워드 입력하고, 키워드에 대한 검색 리스트가 뜨는데 걸리는 시간만큼 기다려준다\n",
    "    time.sleep(1.5)\n",
    "    \n",
    "    search_list = driver.find_elements_by_css_selector('#react-root > section > nav > div._8MQSO.Cx7Bp > div > div > div.LWmhU._0aCwM > div:nth-child(4) > div.drKGC > div > a')\n",
    "    \n",
    "    # 서칭된 a 엘리먼트들 중에 게시물 갯수가 가장 많은 a 엘리먼트를 뽑는다\n",
    "    number_list = []\n",
    "\n",
    "    for element in search_list:\n",
    "        splited_element = element.text.split('게시물')\n",
    "        if len(splited_element) == 2:\n",
    "            number_list.append(int(re.sub(\",\", \"\", splited_element[1])))\n",
    "        else:\n",
    "            number_list.append(0)\n",
    "\n",
    "    max_number = max(number_list)\n",
    "    click_index = number_list.index(max_number)\n",
    "    \n",
    "    # 클릭하고 렌더링이 완료되도록 조금 기다리기\n",
    "    search_list[click_index].click()\n",
    "    time.sleep(1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음 렌더링된 화면의 24개의 게시물을 크롤링하는 함수\n",
    "\n",
    "\n",
    "def initial_crawling():\n",
    "    urls = driver.find_elements_by_xpath(\n",
    "        '//*[@id=\"react-root\"]/section/main/article/div[2]/div/div/div/a')\n",
    "    urls = [url.get_attribute(\"href\") for url in urls]\n",
    "\n",
    "    hash_tag_df = pd.DataFrame(columns=['time', 'hash_tag'])\n",
    "\n",
    "    for i in range(len(urls)):\n",
    "        req = requests.get(urls[i])\n",
    "        response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")\n",
    "        hash_tag_list = response.css(\"meta[property='instapp:hashtags']\")\n",
    "        hash_tag_list = ['#' + hash_tag.attrib['content']\n",
    "                         for hash_tag in hash_tag_list]\n",
    "        hash_tags = ', '.join(hash_tag_list)\n",
    "\n",
    "        data = [\n",
    "            {'time': 0, 'hash_tag': hash_tags}\n",
    "        ]\n",
    "        df = pd.DataFrame(data)\n",
    "        hash_tag_df = hash_tag_df.append(df)\n",
    "\n",
    "    return hash_tag_df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스크롤내리면서 12개씩 추가되는 게시물을 크롤링하는 함수 (두번째만 실행) - chrome 전체화면 기준\n",
    "# 현재 브라우저 화면 크기에 따라 추가적으로 불러오는 div 갯수가 달라짐\n",
    "# 전체화면 기준 div의 urls 총 갯수 = 24 -> 36 -> 45 -> 45 -> ...\n",
    "\n",
    "def second_crawling():\n",
    "    for i in range(0, 1):\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "        time.sleep(1.5)\n",
    "\n",
    "        urls = driver.find_elements_by_xpath(\n",
    "            '//*[@id=\"react-root\"]/section/main/article/div[2]/div/div/div/a')\n",
    "        urls = [url.get_attribute(\"href\") for url in urls]\n",
    "\n",
    "        hash_tag_df = pd.DataFrame(columns=['time', 'hash_tag'])\n",
    "\n",
    "        for i in range(len(urls) - 12, len(urls)):\n",
    "            req = requests.get(urls[i])\n",
    "            response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")\n",
    "            hash_tag_list = response.css(\"meta[property='instapp:hashtags']\")\n",
    "            hash_tag_list = ['#' + hash_tag.attrib['content']\n",
    "                             for hash_tag in hash_tag_list]\n",
    "            hash_tags = ', '.join(hash_tag_list)\n",
    "\n",
    "            data = [\n",
    "                {'time': 0, 'hash_tag': hash_tags}\n",
    "            ]\n",
    "            df = pd.DataFrame(data)\n",
    "            hash_tag_df = hash_tag_df.append(df)\n",
    "\n",
    "    return hash_tag_df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세번째부터는 이 함수 반복하기 - 게시물 9개씩 크롤링\n",
    "\n",
    "def repeat_crawling():\n",
    "    for i in range(0, 1):\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "        time.sleep(1.5)\n",
    "\n",
    "        urls = driver.find_elements_by_xpath(\n",
    "            '//*[@id=\"react-root\"]/section/main/article/div[2]/div/div/div/a')\n",
    "        urls = [url.get_attribute(\"href\") for url in urls]\n",
    "\n",
    "        hash_tag_df = pd.DataFrame(columns=['time', 'hash_tag'])\n",
    "\n",
    "        for i in range(len(urls) - 9, len(urls)):\n",
    "            req = requests.get(urls[i])\n",
    "            response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")\n",
    "            hash_tag_list = response.css(\"meta[property='instapp:hashtags']\")\n",
    "            hash_tag_list = ['#' + hash_tag.attrib['content']\n",
    "                             for hash_tag in hash_tag_list]\n",
    "            hash_tags = ', '.join(hash_tag_list)\n",
    "\n",
    "            data = [\n",
    "                {'time': 0, 'hash_tag': hash_tags}\n",
    "            ]\n",
    "            df = pd.DataFrame(data)\n",
    "            hash_tag_df = hash_tag_df.append(df)\n",
    "\n",
    "    return hash_tag_df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스로 모듈화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile insta_crawling.py\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from scrapy.http import TextResponse\n",
    "# import seaborn as sns\n",
    "# import getpass\n",
    "# import time\n",
    "\n",
    "\n",
    "class instagram_crawling():\n",
    "    \n",
    "    driver = \"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        login_url = \"https://www.instagram.com/accounts/login/?source=auth_switcher\"\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.get(login_url)\n",
    "    \n",
    "    \n",
    "    # 로그인 함수 (seleniun webdriver)\n",
    "    def login(self):\n",
    "\n",
    "        my_id = input('id를 입력하세요: ')\n",
    "        my_password = getpass.getpass('password를 입력하세요: ')\n",
    "\n",
    "        self.driver.find_element_by_css_selector('._2hvTZ.pexuQ.zyHYP[type=text]').send_keys(my_id)\n",
    "        self.driver.find_element_by_css_selector('._2hvTZ.pexuQ.zyHYP[type=password]').send_keys(my_password)\n",
    "        self.driver.find_element_by_css_selector('.sqdOP.L3NKy.y3zKF[type=submit]').click()\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        # 알림설정 하라는 modal창이 뜨는경우 '나중에 하기'를 클릭하는 코드\n",
    "        alert_modal = self.driver.find_element_by_css_selector('body > div.RnEpo.Yx5HN > div > div')\n",
    "        if alert_modal:\n",
    "            self.driver.find_element_by_css_selector('body > div.RnEpo.Yx5HN > div > div > div.mt3GC > button.aOOlW.HoLwm').click()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 검색어를 입력하고 게시물이 가장 많은 검색결과를 클릭 (seleniun webdriver)\n",
    "    def crawling_keyword(self, word):\n",
    "        keyword = word\n",
    "\n",
    "        self.driver.find_element_by_css_selector('#react-root > section > nav > div._8MQSO.Cx7Bp > div > div > div.LWmhU._0aCwM > input').send_keys(keyword)\n",
    "\n",
    "        # 키워드 입력하고, 키워드에 대한 검색 리스트가 뜨는데 걸리는 시간만큼 기다려준다\n",
    "        time.sleep(1.5)\n",
    "\n",
    "        search_list = self.driver.find_elements_by_css_selector('#react-root > section > nav > div._8MQSO.Cx7Bp > div > div > div.LWmhU._0aCwM > div:nth-child(4) > div.drKGC > div > a')\n",
    "\n",
    "        # 서칭된 a 엘리먼트들 중에 게시물 갯수가 가장 많은 a 엘리먼트를 뽑는다\n",
    "        number_list = []\n",
    "\n",
    "        for element in search_list:\n",
    "            splited_element = element.text.split('게시물')\n",
    "            if len(splited_element) == 2:\n",
    "                number_list.append(int(re.sub(\",\", \"\", splited_element[1])))\n",
    "            else:\n",
    "                number_list.append(0)\n",
    "\n",
    "        max_number = max(number_list)\n",
    "        click_index = number_list.index(max_number)\n",
    "\n",
    "        # 클릭하고 렌더링이 완료되도록 조금 기다리기\n",
    "        search_list[click_index].click()\n",
    "        time.sleep(1.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 처음 렌더링된 화면의 24개의 게시물을 크롤링하는 함수\n",
    "    def initial_crawling(self):\n",
    "        urls = self.driver.find_elements_by_xpath('//*[@id=\"react-root\"]/section/main/article/div[2]/div/div/div/a')\n",
    "        urls = [url.get_attribute(\"href\") for url in urls]\n",
    "\n",
    "        hash_tag_df = pd.DataFrame(columns=['time', 'hash_tag'])\n",
    "\n",
    "        for i in range(len(urls)):\n",
    "            req = requests.get(urls[i])\n",
    "            response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")\n",
    "            hash_tag_list = response.css(\"meta[property='instapp:hashtags']\")\n",
    "            hash_tag_list = ['#' + hash_tag.attrib['content'] for hash_tag in hash_tag_list]\n",
    "            hash_tags = ', '.join(hash_tag_list)\n",
    "\n",
    "            data = [\n",
    "                {'time': 0, 'hash_tag': hash_tags}\n",
    "            ]\n",
    "            df = pd.DataFrame(data)\n",
    "            hash_tag_df = hash_tag_df.append(df)\n",
    "\n",
    "        return hash_tag_df.reset_index().drop(columns=['index'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 스크롤내리면서 12개씩 추가되는 게시물을 크롤링하는 함수 (두번째만 실행) - chrome 전체화면 기준\n",
    "    # 현재 브라우저 화면 크기에 따라 추가적으로 불러오는 div 갯수가 달라짐\n",
    "    # 전체화면 기준 div의 urls 총 갯수 = 24 -> 36 -> 45 -> 45 -> ...\n",
    "    def second_crawling(self):\n",
    "        for i in range(0, 1):\n",
    "            self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "            time.sleep(1.5)\n",
    "\n",
    "            urls = self.driver.find_elements_by_xpath('//*[@id=\"react-root\"]/section/main/article/div[2]/div/div/div/a')\n",
    "            urls = [url.get_attribute(\"href\") for url in urls]\n",
    "\n",
    "            hash_tag_df = pd.DataFrame(columns=['time', 'hash_tag'])\n",
    "\n",
    "            for i in range(len(urls) - 12, len(urls)):\n",
    "                req = requests.get(urls[i])\n",
    "                response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")\n",
    "                hash_tag_list = response.css(\"meta[property='instapp:hashtags']\")\n",
    "                hash_tag_list = ['#' + hash_tag.attrib['content'] for hash_tag in hash_tag_list]\n",
    "                hash_tags = ', '.join(hash_tag_list)\n",
    "\n",
    "                data = [\n",
    "                    {'time': 0, 'hash_tag': hash_tags}\n",
    "                ]\n",
    "                df = pd.DataFrame(data)\n",
    "                hash_tag_df = hash_tag_df.append(df)\n",
    "\n",
    "        return hash_tag_df.reset_index().drop(columns=['index'])\n",
    "\n",
    "\n",
    "\n",
    "    # 세번째부터는 이 함수 반복하기 - 게시물 9개씩 크롤링\n",
    "    def repeat_crawling(self):\n",
    "        for i in range(0, 1):\n",
    "            self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "            time.sleep(1.5)\n",
    "\n",
    "            urls = self.driver.find_elements_by_xpath('//*[@id=\"react-root\"]/section/main/article/div[2]/div/div/div/a')\n",
    "            urls = [url.get_attribute(\"href\") for url in urls]\n",
    "\n",
    "            hash_tag_df = pd.DataFrame(columns=['time', 'hash_tag'])\n",
    "\n",
    "            for i in range(len(urls) - 9, len(urls)):\n",
    "                req = requests.get(urls[i])\n",
    "                response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")\n",
    "                hash_tag_list = response.css(\"meta[property='instapp:hashtags']\")\n",
    "                hash_tag_list = ['#' + hash_tag.attrib['content'] for hash_tag in hash_tag_list]\n",
    "                hash_tags = ', '.join(hash_tag_list)\n",
    "\n",
    "                data = [\n",
    "                    {'time': 0, 'hash_tag': hash_tags}\n",
    "                ]\n",
    "                df = pd.DataFrame(data)\n",
    "                hash_tag_df = hash_tag_df.append(df)\n",
    "\n",
    "        return hash_tag_df.reset_index().drop(columns=['index'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
