{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting insta_crawling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile insta_crawling.py\n",
    "import requests\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from scrapy.http import TextResponse\n",
    "import getpass\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "class instagram_crawling():\n",
    "    \n",
    "    driver = \"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        login_url = \"https://www.instagram.com/accounts/login/?source=auth_switcher\"\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.get(login_url)\n",
    "        \n",
    "        \n",
    "    # element의 렌더링을 기다려야 하는 경우, 렌더링 될때까지 재귀적으로 함수 실행  \n",
    "    def check_response(self, webdriver, selector, func , start_time):\n",
    "        try:\n",
    "            if func == 'login':\n",
    "                result = webdriver.find_element_by_css_selector(selector)\n",
    "            elif func == 'input_keyword':\n",
    "                result = webdriver.find_elements_by_css_selector(selector)\n",
    "                while len(result) == 0:\n",
    "                    return self.check_response(self.driver, selector, func, start_time)\n",
    "            elif func == 'initial_crawling':\n",
    "                result = webdriver.find_elements_by_css_selector(selector)\n",
    "                while len(result) != 8:\n",
    "                    return self.check_response(self.driver, selector, func, start_time)\n",
    "                \n",
    "            return result\n",
    "        \n",
    "        except Exception:\n",
    "            # 10초 기다리고 종료\n",
    "            if time.time() - start_time > 10:\n",
    "                print(\"=========== no response in function : {} ===========\".format(func))\n",
    "                return False\n",
    "            return self.check_response(self.driver, selector, func, start_time)\n",
    "        \n",
    "            \n",
    "    \n",
    "    # 로그인 함수 (seleniun webdriver)\n",
    "    def login(self):\n",
    "\n",
    "        my_id = input('id를 입력하세요: ')\n",
    "        my_password = getpass.getpass('password를 입력하세요: ')\n",
    "\n",
    "        self.driver.find_element_by_css_selector('._2hvTZ.pexuQ.zyHYP[type=text]').send_keys(my_id)\n",
    "        self.driver.find_element_by_css_selector('._2hvTZ.pexuQ.zyHYP[type=password]').send_keys(my_password)\n",
    "        self.driver.find_element_by_css_selector('.sqdOP.L3NKy.y3zKF[type=submit]').click()\n",
    "\n",
    "\n",
    "        # 알림설정 하라는 modal창이 뜨는경우 '나중에 하기'를 클릭하는 코드\n",
    "        start_time = time.time()\n",
    "        alert_modal = self.check_response(self.driver, 'body > div.RnEpo.Yx5HN > div > div', 'login', start_time)\n",
    "        if alert_modal:\n",
    "            self.driver.find_element_by_css_selector('body > div.RnEpo.Yx5HN > div > div > div.mt3GC > button.aOOlW.HoLwm').click()\n",
    "        else:\n",
    "            print('================ login error ================')\n",
    "            self.driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "    # 검색어를 입력하고 게시물이 가장 많은 검색결과를 클릭 (seleniun webdriver)\n",
    "    def input_keyword(self, word):\n",
    "        keyword = word\n",
    "\n",
    "        self.driver.find_element_by_css_selector('#react-root > section > nav > div._8MQSO.Cx7Bp > div > div > div.LWmhU._0aCwM > input').send_keys(keyword)\n",
    "\n",
    "        # 키워드 입력하고, 키워드에 대한 검색 리스트가 뜨는데 걸리는 시간만큼 기다려준다\n",
    "        start_time = time.time()\n",
    "        search_list = self.check_response(self.driver, '#react-root > section > nav > div._8MQSO.Cx7Bp > div > div > div.LWmhU._0aCwM > div:nth-child(4) > div.drKGC > div > a', 'input_keyword', start_time)\n",
    "        \n",
    "        \n",
    "        # 서칭된 a 엘리먼트들 중에 게시물 갯수가 가장 많은 a 엘리먼트를 뽑는다\n",
    "        number_list = []\n",
    "\n",
    "        for element in search_list:\n",
    "            splited_element = element.text.split('게시물')\n",
    "            if len(splited_element) == 2:\n",
    "                number_list.append(int(re.sub(\",\", \"\", splited_element[1])))\n",
    "            else:\n",
    "                number_list.append(0)\n",
    "\n",
    "        max_number = max(number_list)\n",
    "        click_index = number_list.index(max_number)\n",
    "\n",
    "        search_list[click_index].click()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 처음 렌더링된 화면의 24개의 게시물을 크롤링하는 함수\n",
    "    def initial_crawling(self):\n",
    "        \n",
    "        # 검색 키워드를 클릭 한 이후 게시물 렌더링이 완료되었는지 확인\n",
    "        start_time = time.time()\n",
    "        self.check_response(self.driver, '#react-root > section > main > article > div:nth-child(3) > div > div', 'initial_crawling', start_time)\n",
    "        \n",
    "        urls = self.driver.find_elements_by_xpath('//*[@id=\"react-root\"]/section/main/article/div[2]/div/div/div/a')\n",
    "        urls = [url.get_attribute(\"href\") for url in urls]\n",
    "\n",
    "        hash_tag_df = pd.DataFrame(columns=['time', 'hash_tag'])\n",
    "\n",
    "        for i in range(len(urls)):\n",
    "            req = requests.get(urls[i])\n",
    "            response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")\n",
    "            hash_tag_list = response.css(\"meta[property='instapp:hashtags']\")\n",
    "            hash_tag_list = ['#' + hash_tag.attrib['content'] for hash_tag in hash_tag_list]\n",
    "            hash_tags = ', '.join(hash_tag_list)\n",
    "\n",
    "            data = [\n",
    "                {'time': 0, 'hash_tag': hash_tags}\n",
    "            ]\n",
    "            df = pd.DataFrame(data)\n",
    "            hash_tag_df = hash_tag_df.append(df)\n",
    "\n",
    "        return hash_tag_df.reset_index().drop(columns=['index'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 스크롤내리면서 12개씩 추가되는 게시물을 크롤링하는 함수 (두번째만 실행) - chrome 전체화면 기준\n",
    "    # 현재 브라우저 화면 크기에 따라 추가적으로 불러오는 div 갯수가 달라짐\n",
    "    # 전체화면 기준 div의 urls 총 갯수 = 24 -> 36 -> 45 -> 45 -> ...\n",
    "    def second_crawling(self):\n",
    "        \n",
    "        hash_tag_df = pd.DataFrame(columns=['time', 'hash_tag'])\n",
    "        \n",
    "        for i in range(0, 1):\n",
    "            self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "            time.sleep(1.5)\n",
    "\n",
    "            urls = self.driver.find_elements_by_xpath('//*[@id=\"react-root\"]/section/main/article/div[2]/div/div/div/a')\n",
    "            urls = [url.get_attribute(\"href\") for url in urls]\n",
    "\n",
    "            for i in range(len(urls) - 12, len(urls)):\n",
    "                req = requests.get(urls[i])\n",
    "                response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")\n",
    "                hash_tag_list = response.css(\"meta[property='instapp:hashtags']\")\n",
    "                hash_tag_list = ['#' + hash_tag.attrib['content'] for hash_tag in hash_tag_list]\n",
    "                hash_tags = ', '.join(hash_tag_list)\n",
    "\n",
    "                data = [\n",
    "                    {'time': 0, 'hash_tag': hash_tags}\n",
    "                ]\n",
    "                df = pd.DataFrame(data)\n",
    "                hash_tag_df = hash_tag_df.append(df)\n",
    "\n",
    "        return hash_tag_df.reset_index().drop(columns=['index'])\n",
    "\n",
    "\n",
    "\n",
    "    # 세번째부터는 이 함수 반복하기 - 게시물 9개씩 크롤링\n",
    "    def repeat_crawling(self):\n",
    "        \n",
    "        hash_tag_df = pd.DataFrame(columns=['time', 'hash_tag'])\n",
    "        \n",
    "        for i in range(0, 1):\n",
    "            self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "            time.sleep(1.5)\n",
    "\n",
    "            urls = self.driver.find_elements_by_xpath('//*[@id=\"react-root\"]/section/main/article/div[2]/div/div/div/a')\n",
    "            urls = [url.get_attribute(\"href\") for url in urls]\n",
    "\n",
    "            for i in range(len(urls) - 9, len(urls)):\n",
    "                req = requests.get(urls[i])\n",
    "                response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")\n",
    "                hash_tag_list = response.css(\"meta[property='instapp:hashtags']\")\n",
    "                hash_tag_list = ['#' + hash_tag.attrib['content'] for hash_tag in hash_tag_list]\n",
    "                hash_tags = ', '.join(hash_tag_list)\n",
    "\n",
    "                data = [\n",
    "                    {'time': 0, 'hash_tag': hash_tags}\n",
    "                ]\n",
    "                df = pd.DataFrame(data)\n",
    "                hash_tag_df = hash_tag_df.append(df)\n",
    "\n",
    "        return hash_tag_df.reset_index().drop(columns=['index'])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# instagram_crawling 클래스를 이용해서 해시태그를 크롤링하고 전체 데이터프레임을 리턴한다\n",
    "def crawling_start(keyword, repeat_num, mongo_save=0):\n",
    "    \"\"\"\n",
    "    3 input arguments\n",
    "    keyword: \n",
    "        검색어(주제)\n",
    "    repeat_num: \n",
    "        세번째 크롤링 함수의 반복 횟수\n",
    "        ex) repeat_num = 2 -> 데이터 row 갯수 = 24 + 12 + (9 * 2)\n",
    "            repeat_num = 3 -> 데이터 row 갯수 = 24 + 12 + (9 * 3)\n",
    "            repeat_num = 4 -> 데이터 row 갯수 = 24 + 12 + (9 * 4)\n",
    "    mongo_save:\n",
    "        aws 서버의 mongodb 데이터베이스에 저장 할 건지 결정하는 인자\n",
    "        저장 = 1\n",
    "        저장 안함 = 0\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    keyword = keyword\n",
    "    num = repeat_num\n",
    "    result_df = pd.DataFrame(columns=['time', 'hash_tag'])\n",
    "    \n",
    "    insta = instagram_crawling()\n",
    "    \n",
    "    insta.login()\n",
    "    insta.input_keyword(keyword)\n",
    "    \n",
    "    initial_df = insta.initial_crawling()\n",
    "    result_df = pd.concat([result_df, initial_df])\n",
    "    \n",
    "    second_df = insta.second_crawling()\n",
    "    result_df = pd.concat([result_df, second_df])\n",
    "    \n",
    "    # 마지막 크롤링 함수 반복\n",
    "    for i in range(0, num):\n",
    "        try:\n",
    "            df = insta.repeat_crawling()\n",
    "            result_df = pd.concat([result_df, df])\n",
    "        except Exception as exc:\n",
    "            print('에러가 발생했습니다 : ', exc)\n",
    "    \n",
    "    result_df = result_df.reset_index().drop(columns=['index'])\n",
    "    \n",
    "    \n",
    "    # 세번째 인자를 1로 넘겨주면, aws mongodb에 데이터를 저장한다\n",
    "    if mongo_save == 1:\n",
    "        mongo_df_list = result_df.to_dict(\"records\")\n",
    "        client = pymongo.MongoClient('mongodb://root:dss@13.124.100.70:27017')\n",
    "        db = client.insta_crawling\n",
    "        collection = db.data\n",
    "\n",
    "        for i in range(len(mongo_df_list)):\n",
    "            collection.insert(mongo_df_list[i])\n",
    "    \n",
    "    return result_df.drop(columns=['time'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
